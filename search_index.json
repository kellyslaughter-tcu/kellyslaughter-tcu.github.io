[
["data.html", "2 Obtaining External Data 2.1 Financial Data 2.2 Social Media Data 2.3 Economic Data", " 2 Obtaining External Data We examine economic and social measures that vary with the price of gold. We speculate that good economic information results in lower gold prices and vice versa, under the premise that gold is seen as a store of value in uncertain times. We track changes in the price of gold to changes in the daily price of Tripwire (as unexpected economic news may curtail or encourage travel), monthly unemployment insurance, Tweets about vacations, and Reddit posts from the Stock Picks subreddit that reference the term ‘gold.’ To obtain this data via: The R package Quandl(Raymond McTaggart, Gergely Daroczi, and Clement Leung 2019) for commodity data (i.e., gold prices) The R package quantmod(Ryan and Ulrich 2020) for stock data (i.e., Tripwire) The R package rtweet(Kearney 2020) for Twitter data and the R package RedditExtractoR(Rivera 2019) for Reddit data The R package httr(Wickham 2019) for the Bureau of Economic Analysis data (i.e., unemployment insurance) Other sources of data ‘friendly’ to R can be found at [ComputerWorld R Packages for Data] (https://www.computerworld.com/article/3109890/these-r-packages-import-sports-weather-stock-data-and-more.html). Other general sources of data available through API’s can be found at https://www.programmableweb.com/apis/directory. 2.1 Financial Data 2.1.1 Commodity Prices Quandl is one of the most popular data sources that offers an API for data. Data sources available via API include:Commodity Prices, Stock Prices, Currency Prices, and Bitcoin Create an account at https://www.quandl.com/sign-up if you plan on using Quandl calls more than fifty times a day - Quandl will provide an API key. We use the function Quandl available after the Quandl library is loaded to obtain gold prices from April 15 to the most recent date. Note that the api_key is set to Sys.getenv(&quot;QUANDL_KEY&quot;). It is bad practice to include the key in the code, so I have loaded the API key provided by Quandl into a systems variable stored in the project file .Renviron and retrieve it for use from the systems variables. .Renviron can be maintained via the usethis package using the command usethis::edit_r_makevars()). Warning: Most API’s have limits for use (by frequency, volume, rows, etc.). Sometimes your data will not appear to change after calls due to this limitation. library(Quandl) London_Gold_AM = Quandl(&quot;LBMA/GOLD&quot;, start_date=&quot;2020-04-15&quot;, api_key = Sys.getenv(&quot;QUANDL_KEY&quot;)) The data is returned in the form of a data frame, which is suitable for most uses in R.The financial data is then displayed in the Table 2.1. Table 2.1: First Few Rows of Financial Data Set Quandl Date USD (AM) USD (PM) GBP (AM) GBP (PM) EURO (AM) EURO (PM) 2020-05-19 1735.25 1737.95 1416.14 1418.34 1584.11 1589.01 2020-05-18 1756.90 1734.70 1450.85 1423.15 1625.84 1597.66 2020-05-15 1734.85 1735.35 1422.06 1427.67 1604.39 1602.60 2020-05-14 1716.40 1731.60 1403.67 1420.09 1587.84 1603.98 2020-05-13 1699.85 1708.40 1383.85 1394.74 1568.11 1573.09 2020-05-12 1703.45 1702.40 1381.84 1379.80 1574.50 1565.87 2.1.2 Stock Prices We will use the quantmod R package to retrieve stock price data. We pass the stock price symbols and data ranges to Yahoo which returns the financial data. In our case, we ask for S&amp;P 500, Kelly Temp Services, Tripwire, and GrubHub stock prices from fifteen days (Sys.Date() - 15) ago to today (Sys.Date()). No API key is required. Note that quantmod relies on additional R packages - R will download these for us automatically. library(quantmod) # retrieve financial data (used for function via getSymbols) (masks as.Date from base R) stock_data &lt;- getSymbols(c(&quot;^GSPC&quot;, &quot;KELYA&quot;, &quot;TRIP&quot;, &quot;GRUB&quot;), # KELYA = Kelly Services, TRIP = Trip Advisor, GRUB = Grubhub src = &#39;yahoo&#39;, from = (Sys.Date() - 15), to = Sys.Date()) Unlike the data returned via Quandl, the data returned from quantmod needs some manipulation to be ready for use. As we asked for four different prices, quantmod returned a structure with four sets of data nested within. Execute the command head(stock_data) in the Console to see the four structures’ names. We will load three libraries - dplyr(Wickham et al. 2020), broom(Robinson and Hayes 2020), and tidyr(Wickham and Henry 2020) - that will allow us to extract and manipulate the data into a form usable for analysis. We first pull the Tripwire data from the stock_data structure via the tidy command, then convert the data from a ‘long’ format to a ‘wide’ format via the spread command. library(broom) # tidy function library(tidyr) # spread function TRIP_tidy &lt;- tidy(TRIP) # tidy from broom package converts list to &quot;long&quot; data frame for DJI TRIP_wide &lt;- spread(TRIP_tidy, series, value) # spread from tidyr; converts long to wide using series values for new columns; that is, turn series row values to new columns We want to explore not the absolute price of Tripwire stock on a given day but the difference between its high and low price. We will need a new variable we will call Trip.Diff (to stay consistent with the naming convention of quantmod) and add this new variable to the data set via the mutate function provided through the dplyr package. library(dplyr) # for mutate function (may already be loaded, but loading twice will not create an error) # The change in price during the day may be of interest TRIP_wide &lt;- TRIP_wide %&gt;% mutate(TRIP.Diff = TRIP.High - TRIP.Low) The first few rows of the Tripwire data are displayed in Table 2.2 as rendered through the package knitr(Xie 2020b). Table 2.2: First Few Rows of Financial Data Set index TRIP.Adjusted TRIP.Close TRIP.High TRIP.Low TRIP.Open TRIP.Volume TRIP.Diff 2020-05-05 18.36 18.36 19.32 18.08 19.13 2779100 1.240000 2020-05-06 18.09 18.09 18.59 17.58 18.14 2541300 1.010000 2020-05-07 18.72 18.72 18.92 18.31 18.55 3537000 0.610001 2020-05-08 17.91 17.91 18.90 17.50 18.05 6450900 1.400000 2020-05-11 16.70 16.70 17.78 16.67 17.78 4766500 1.110001 2020-05-12 16.90 16.90 17.27 16.62 16.78 3743300 0.649999 2.2 Social Media Data 2.2.1 Twitter Twitter requires registration in the form of an ‘application’ to use its API. Tweets that are public and can be searched via an API by keyword or account. Twitter does impose restrictions on the API use, e.g., limiting the number of calls per minute and the number of tweets returned. Twitter uses OAuth 1.0 tokens for authorization. You will need a consumer key, a consumer secret, an access token, and an access secret (basically, you need five character strings). Set up instructions can be found at https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html. We will use the command search_tweets via the rtweet package to retrieve Twitter data by keywords. If you select a very broad term for search (e.g., “election”) you might only retrieve data from the last few hours. If you select a more obscure term (e.g., “kakorrhaphiophobia”) you still can only retrieve Tweets up to seven days old when using the free Twitter account. We first set parameter values for the call to search_tweets, using the same technique for loading the token values, setting the search string to Caribbean and / or vacation, and limit the number of returned tweets to 1,000. The data returned is accessible through the vacation_tweets variable. library(httpuv) # for browser based twitter authentication library(rtweet) twitter_token &lt;- create_token( app = Sys.getenv(&quot;TWITTER_app&quot;), consumer_key = Sys.getenv(&quot;TWITTER_consumer_key&quot;), consumer_secret = Sys.getenv(&quot;TWITTER_consumer_secret&quot;), access_token = Sys.getenv(&quot;TWITTER_access_token&quot;), access_secret = Sys.getenv(&quot;TWITTER_access_secret&quot;)) tweet_search_string &lt;- &#39;Caribbean vacation &#39; # space for AND; this string also for hashtag terms and can include logic such as min_retweets:50 to return tweets that were retweeted a least 50 times Kmax_number_of_tweets = 1000 # Up to 18,000 vacation_tweets &lt;- search_tweets( q = tweet_search_string, n = Kmax_number_of_tweets, # Up to 18000 every 15 minutes include_rts = FALSE, # Retweets = tweet generated by &quot;retweet&quot; (recycle arrows), not quotes entering &quot;RT&quot; into text of one&#39;s tweets lang = &quot;en&quot;, # language: BCP 47 language identifier geocode = lookup_coords(&quot;usa&quot;), # for geo enabled tweets, most tweets do not include a geo code # since = from, # until = Sys.Date(), retryonratelimit = FALSE) # If ask for &gt; 18k if TRUE, will wait and resend request when eligible for next batch The Twitter API returns dozens of fields as shown below. While we will concentrate only on the actual tweet text, other fields that might be of interest is the number of retweets or the number of account followers. names(vacation_tweets) ## [1] &quot;user_id&quot; &quot;status_id&quot; ## [3] &quot;created_at&quot; &quot;screen_name&quot; ## [5] &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; ## [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; ## [11] &quot;is_quote&quot; &quot;is_retweet&quot; ## [13] &quot;favorite_count&quot; &quot;retweet_count&quot; ## [15] &quot;quote_count&quot; &quot;reply_count&quot; ## [17] &quot;hashtags&quot; &quot;symbols&quot; ## [19] &quot;urls_url&quot; &quot;urls_t.co&quot; ## [21] &quot;urls_expanded_url&quot; &quot;media_url&quot; ## [23] &quot;media_t.co&quot; &quot;media_expanded_url&quot; ## [25] &quot;media_type&quot; &quot;ext_media_url&quot; ## [27] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; ## [29] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; ## [31] &quot;mentions_screen_name&quot; &quot;lang&quot; ## [33] &quot;quoted_status_id&quot; &quot;quoted_text&quot; ## [35] &quot;quoted_created_at&quot; &quot;quoted_source&quot; ## [37] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; ## [39] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; ## [41] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; ## [43] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; ## [45] &quot;quoted_location&quot; &quot;quoted_description&quot; ## [47] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; ## [49] &quot;retweet_text&quot; &quot;retweet_created_at&quot; ## [51] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; ## [53] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; ## [55] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; ## [57] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; ## [59] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; ## [61] &quot;retweet_description&quot; &quot;retweet_verified&quot; ## [63] &quot;place_url&quot; &quot;place_name&quot; ## [65] &quot;place_full_name&quot; &quot;place_type&quot; ## [67] &quot;country&quot; &quot;country_code&quot; ## [69] &quot;geo_coords&quot; &quot;coords_coords&quot; ## [71] &quot;bbox_coords&quot; &quot;status_url&quot; ## [73] &quot;name&quot; &quot;location&quot; ## [75] &quot;description&quot; &quot;url&quot; ## [77] &quot;protected&quot; &quot;followers_count&quot; ## [79] &quot;friends_count&quot; &quot;listed_count&quot; ## [81] &quot;statuses_count&quot; &quot;favourites_count&quot; ## [83] &quot;account_created_at&quot; &quot;verified&quot; ## [85] &quot;profile_url&quot; &quot;profile_expanded_url&quot; ## [87] &quot;account_lang&quot; &quot;profile_banner_url&quot; ## [89] &quot;profile_background_url&quot; &quot;profile_image_url&quot; For convenience, we will pull the fields created_at, text, favorite_count, and retweet_count into another data set - vacation_tweets_df. #vacation_tweets_df$europe &lt;- grepl(&quot; Europe &quot;, rt$text, ignore.case = TRUE) # flag to indicate text include term &#39;Europe&#39; vacation_tweets_df &lt;- tibble(created_at = vacation_tweets$created_at, text = vacation_tweets$text, favorite_count = vacation_tweets$favorite_count, retweet_count = vacation_tweets$retweet_count) # tibble keeps the text from defaulting to factor Twitter also allows searches by account via the command get_timeline. In the code below (which is not executred), tweets sent by the news service Business Wire would be captured and added to the vacation_tweets_df. NewsFromBW_tweets &lt;- get_timeline(&quot;NewsFromBW&quot;, n = Kmax_number_of_tweets) NewsFromBW_tweets_df &lt;- data.frame(created_at = NewsFromBW_tweets$created_at, text = NewsFromBW_tweets$text, favorite_count = NewsFromBW_tweets$favorite_count, retweet_count = NewsFromBW_tweets$retweet_count) vacation_tweets_df$source &lt;- &quot;General&quot; NewsFromBW_tweets_df$source &lt;- &quot;NewsFromBW&quot; vacation_tweets_df &lt;- rbind(NewsFromBW_tweets_df, vacation_tweets_df) vacation_tweets_df$text &lt;- gsub(&quot; Caribbean&quot;, &quot;&lt;span class=&#39;searchterms&#39;&gt; contract&lt;/span&gt;&quot;, vacation_tweets_df$text) The first few rows of the Twitter data are displayed below. Caution: The Twitter text may include characters R will interpret as escape characters which may adversely affect the bookdown generation. ## # A tibble: 6 x 4 ## created_at text favorite_count retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2020-05-20 20:02:49 &quot;You know what? If things al~ 0 0 ## 2 2020-05-20 19:32:44 &quot;@GayUnclePhil Jelly! June w~ 1 0 ## 3 2020-05-20 19:19:15 &quot;I can’t stop thinking about~ 11 0 ## 4 2020-05-20 19:12:13 &quot;5 Day Eastern Caribbean Fro~ 1 0 ## 5 2020-05-20 18:41:58 &quot;[Race to the Finish] Artee ~ 0 0 ## 6 2020-05-20 18:39:38 &quot;I know I always say this, b~ 0 0 No values appearing in the ‘favorite_count’ and ‘retweet_count’ may indicate values of zero 2.2.2 Reddit Reddit does not require an API key. Using the function get_reddit from the package RedditExtractoR, we will obtain posts from the rubReddit stock picks, searching by the terms gold and _a few other terms silver. library(RedditExtractoR) reddit_data = get_reddit(search_terms = &quot;gold silver&quot;, subreddit = &quot;stock_picks&quot;, sort_by = &quot;new&quot;) # returns text as chr, not factor ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% The first few rows of the Reddit results are displayed below. ## [1] &quot;UBS Wealth Management Determines Ideal Selling Point for Gold:\\n\\nhttp://gold-prediction.com/2016/03/13/gold-news-ubs-wealth-management-determines-ideal-selling-point-for-gold/&quot; ## [2] &quot;Good to see this sub headed in a better direction. Best of luck!&quot; ## [3] &quot;Thanks! Hope to see you in the comments in the future!&quot; ## [4] &quot;Good luck!\\n\\n\\nPick of the month was good fun, I miss it.&quot; ## [5] &quot;Thank you! I&#39;ll restart the stock pick contest at the end of next month with some rules, stat-tracking, and fun flairs as prizes!&quot; ## [6] &quot;Maybe a bot for a monthly reminder message too. I always seem to remember to make my pick on the 2nd or 3rd. &quot; 2.3 Economic Data 2.3.1 Unemployment Insurance While an R BEA package exists for pulling data, I have had some issues using it (it has not been updated in a couple of years; meanwhile, the public sites offering data undergo frequent updates). So instead we will use a more generic solution, a RESTful pull via the R httr package (supporting cURL-like access to sites). We will ‘ask’ that the data is returned in a JSON (vs XML), a format that will need to be changed for use in R. Accordingly, we will use the R package jsonlite to convert the data into a wide format called a data frame or tibble. The documentation for the packages can be found at: https://cran.r-project.org/web/packages/httr/ https://cran.r-project.org/web/packages/jsonlite The BEA data is available free of charge, but to registered users. Thus you will need to provide the unique key assigned to you by BEA as described at https://apps.bea.gov/API/signup/index.cfm. BEA methodologies can be found at https://www.bea.gov/resources/methodologies/nipa-handbook. A description of the data available can be found at https://apps.bea.gov/api/_pdf/bea_web_service_api_user_guide.pdf starting at page 16. The data set categories can be found at https://www.bea.gov/open-data). In the example provided below, we will obtain data from the NIPA database and store it in the variable called BEA_response. We check the return code hoping for a 200, which indicates the technical transaction was successful, but that does not necessarily mean data was returned. library(httr) # Normmally I would load all packages in a single chunk at the begiining of the notebook # Retrieve my BEA API Key stored in .Renviron project file BEA_API_Key &lt;- Sys.getenv(&quot;BEA_KEY&quot;) # Set parameter values for call BEA_Data_Set_Name &lt;- &quot;NIPA&quot; # also &quot;NIUnderlingDetail&quot;, &quot;FixedAssets&quot;, &quot;MNE&quot;, &quot;GDPbyIndustry&quot;, &quot;ITA&quot; , &quot;IIP&quot;, &quot;InputOutput&quot; , &quot;UnderlyingGDPbyIndustry&quot; BEA_Table_Name &lt;- &quot;T20600&quot; # Table T20600, Line 21 is Unemployment Insurance BEA_Year &lt;- &quot;2020&quot; BEA_Frequency = &quot;M&quot; # A = Annual, Q = Quarterly, M = Monthly - can use combination BEA_Results_Type &lt;- &quot;JSON&quot; # BEA_Data_Set_Name &lt;- &quot;NIUnderlyingDetail&quot; # BEA_Table_Name &lt;- &quot;U70205S&quot; # Line 12 is Truck Sales BEA_string &lt;- paste0(&quot;https://apps.bea.gov/api/data/?&amp;UserID=&quot;, BEA_API_Key, &quot;&amp;method=GetData&quot;, &quot;&amp;DataSetName=&quot;, BEA_Data_Set_Name, &quot;&amp;TableName=&quot;, BEA_Table_Name, &quot;&amp;Frequency=&quot;, BEA_Frequency, &quot;&amp;Year=&quot;, BEA_Year, &quot;&amp;ResultFormat=&quot;, BEA_Results_Type) # The parameters TableName, Frequency, and Year are required for the NIPA data sets. Other BEA data sets may require different parameters BEA_response &lt;- GET(BEA_string) # GET passes the string we created to the BEA web site. THe data set results returned are placed into the BEA_response variable http_status(BEA_response) # We Want code 200, which indicates successfull technical call, but not that data was returned ## $category ## [1] &quot;Success&quot; ## ## $reason ## [1] &quot;OK&quot; ## ## $message ## [1] &quot;Success: (200) OK&quot; Now we reformat the JSON data to a form usable by most R packages. library(jsonlite) BEA_response_content &lt;- content(BEA_response, &quot;text&quot;) # the content command &#39;is&#39;pulls&#39; the data from results BEA_response_struct &lt;- fromJSON(BEA_response_content) # Convert the JSON into a table-like format # THe BEA_response_struct variable is an elaborate structure that includes data and metadata (use the command str(BEA_response_struct) to see the structure). We just want to data. So we reference the data into a new variable. BEA_tibble &lt;- tibble(BEA_response_struct$BEAAPI$Results$Data) # Capture only the part of the table we want Table T20600 from the NIPA database includes a number of variables. We will pull the unemployment insurance (designated byLineNumber = 21) into a new variable called BEA_tibble_unemployment_ins. BEA_tibble_unemployment_ins &lt;- filter(BEA_tibble, LineNumber == 21) # Unemployment Insurance, filter is a dplyr function The first few rows of the BEA results are displayed in Table 2.3. Table 2.3: First Few Rows of Economic Data Set TableName SeriesCode LineNumber LineDescription TimePeriod METRIC_NAME CL_UNIT UNIT_MULT DataValue NoteRef T20600 W825RC 21 Unemployment insurance 2020M01 Current Dollars Level 6 26,398 NA T20600 W825RC 21 Unemployment insurance 2020M02 Current Dollars Level 6 26,153 NA T20600 W825RC 21 Unemployment insurance 2020M03 Current Dollars Level 6 65,290 NA References "]
]
