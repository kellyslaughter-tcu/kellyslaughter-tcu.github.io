[
["index.html", "Neeley BIS Data Science CompetitionSummer 2020 1 Overview", " Neeley BIS Data Science CompetitionSummer 2020 Kelly T Slaughter 2020-06-19 1 Overview rmd version of file Welcome to the inaugural Neeley BIS Data Science Competition! Teams and individuals will research relationships between social media and economic or financial metrics and present the findings visually. The social media content may be related to a (set of) persons or terms. The economic data may be consumer spending and borrowing, stock prices, manufacturing output, etc. Competitors may want to first brainstorm about the relationships they are seeking to uncover, informally examine the relationships to see if anything of interest exists, then develop a plan for developing the more rigorous analysis and supporting documents. In this online book, I will share a ‘fake’ example of an entry. The code behind each chapter will be explained in our Zoom meetings as documented in Table 1.1. Table 1.1: Data Science Competition Zoom Meeting Schedule Date Topic Link 2020-05-21 Kick-off https://tcu.zoom.us/j/97403249734 2020-05-28 External Data https://tcu.zoom.us/j/97427925678 2020-06-04 Text Analysis https://tcu.zoom.us/j/93233516653 2020-06-11 Open https://tcu.zoom.us/j/99538834884 2020-06-18 Visualization https://tcu.zoom.us/j/94416905185 2020-07-09 Open https://tcu.zoom.us/j/93574469036 2020-07-16 Publishing https://tcu.zoom.us/j/92745655708 2020-07-23 Open https://tcu.zoom.us/j/98806302340 all times are 6:00 PM Central The projects should be completed and available for viewing by August 6. Before the May 28 meeting: Download R (https://cran.r-project.org/) Download RStudio (https://rstudio.com/products/rstudio/download/) Install the tidyverse suite of packages using the command install.packages(\"tidyverse\") Included in the suite is the **ggplot2*`** package that we will use for visualizations Have one person from your team create an account on GitHub (to use as a host for your final work product) Brainstorm on the type of relationship you want to explore and publish. I will demonstrate how to obtain data from the following sources: Stock price data from Yahoo Install the quantmod package via install.packages(\"quantmod\") Commodity prices for gold from Quandl Install the Quandl package via install.packages(\"Quandl\") Create an account at https://www.quandl.com/sign-up through which you will be provided an API key Social media data from Twitter Install the rtweet package via install.packages(\"rtweet\") Install the httpuv package via install.packages(\"httpuv\") Create an account / application as explained at https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html. After creatinging an account, you will have values for an application name, consumer key, consumer secret, access token, and access secret Social media data from Reddit's Stock Picks subreddit Install the RedditExtractoR package via install.packages(\"RedditExtractoR\") Economic data from the Bureau of Economic Analysis [BEA] (https://www.bea.gov/) You will need an API key as explained at https://apps.bea.gov/API/signup/index.cfm. While there is an R package available for acquiring BEA data, it has not been updated in a couple of years and it appears that a few commands may not be working. So we will use a direct RESTful call (i.e., GET) via the httr package via install.packages(\"httr\"). Also install the jsonlite package. This package supports the manipulation of external data received as JSON. "],
["data.html", "2 Obtaining External Data 2.1 Primary Functions 2.2 Financial Data 2.3 Economic Data", " 2 Obtaining External Data rmd version of file We examine economic and social measures that vary with the price of gold. We speculate that good economic information results in lower gold prices and vice versa, under the premise that gold is seen as a store of value in uncertain times. We track changes in the price of gold to changes in the daily price of Tripwire (as unexpected economic news may curtail or encourage travel), monthly unemployment insurance, Tweets about vacations, and Reddit posts from the Stock Picks subreddit that reference the term ‘gold.’ We obtain this data via: The R package Quandl(Raymond McTaggart, Gergely Daroczi, and Clement Leung 2019) for commodity data (i.e., gold prices) The R package quantmod(Ryan and Ulrich 2020) for stock data (i.e., Tripwire) The R package rtweet(Kearney 2020) for Twitter data and the R package RedditExtractoR(Rivera 2019) for Reddit data The R package httr(Wickham 2019a) for the Bureau of Economic Analysis data (i.e., unemployment insurance) Other sources of data ‘friendly’ to R can be found at ComputerWorld R Packages for Data. Other general sources of data available through API’s can be found at https://www.programmableweb.com/apis/directory. 2.1 Primary Functions Quandl() via the Quandl package: Retrieve commodity price data getSymbols() via the quantmod package: Retrieve stock price data search_tweets() via the rtweet package: Retrieve Twitter data get_reddit() via the package RedditExtractoR: Retrieve Reddit data GET() via the httr package (supporting cURL-like access to sites): Retrieve economic data fromJSON() via the package jsonlite: Convert retrieved JSON data into a tibble 2.2 Financial Data 2.2.1 Commodity Prices Quandl is one of the most popular data sources that offers an API for data. Data sources available via API include:Commodity Prices, Stock Prices, Currency Prices, and Bitcoin Create an account at https://www.quandl.com/sign-up if you plan on using Quandl calls more than fifty times a day - Quandl will provide an API key. We use the function Quandl() available after the Quandl package is loaded to obtain gold prices from April 15 to the most recent date. Note that the api_key is set to Sys.getenv(\"QUANDL_KEY\"). It is bad practice to include the key in the code, so I have loaded the API key provided by Quandl into a systems variable stored in the project file .Renviron and retrieve it for use from the systems variables. .Renviron can be maintained via the usethis package using the command usethis::edit_r_makevars()). Warning: Most API’s have limits for use (by frequency, volume, rows, etc.). Sometimes your data will not appear to change after calls due to this limitation. library(Quandl) London_Gold_AM &lt;- Quandl(code = &quot;LBMA/GOLD&quot;, start_date = &quot;2020-04-15&quot;, api_key = Sys.getenv(&quot;QUANDL_KEY&quot;)) The data is returned in the form of a data frame, which is suitable for most uses in R.The financial data is then displayed in the Table 2.1. Table 2.1: First Few Rows of Financial Data Set Quandl Date USD (AM) USD (PM) GBP (AM) GBP (PM) EURO (AM) EURO (PM) 2020-06-18 1732.65 1719.50 1384.73 1383.51 1539.29 1532.93 2020-06-17 1717.30 1724.35 1368.69 1375.17 1527.88 1537.26 2020-06-16 1728.35 1719.85 1366.61 1361.78 1525.44 1526.54 2020-06-15 1710.40 1710.45 1365.58 1361.52 1520.72 1516.83 2020-06-12 1735.85 1733.50 1374.10 1378.13 1533.28 1534.15 2020-06-11 1731.90 1738.25 1361.79 1373.74 1519.57 1528.10 2.2.2 Stock Prices We will use the quantmod package to retrieve stock price data. We pass the stock price symbols and data ranges to Yahoo which returns the financial data. In our case, we ask for S&amp;P 500, Kelly Temp Services, Tripwire, and GrubHub stock prices from fifteen days (Sys.Date() - 15) ago to today (Sys.Date()). No API key is required. Note that quantmod relies on additional R packages - R will download these for us automatically. library(quantmod) # retrieve financial data (used for function via getSymbols) (masks as.Date from base R) getSymbols(c(&quot;^GSPC&quot;, &quot;KELYA&quot;, &quot;TRIP&quot;, &quot;GRUB&quot;), # KELYA = Kelly Services, TRIP = Trip Advisor, GRUB = Grubhub src = &#39;yahoo&#39;, from = (Sys.Date() - 15), to = Sys.Date()) ## [1] &quot;^GSPC&quot; &quot;KELYA&quot; &quot;TRIP&quot; &quot;GRUB&quot; Unlike the data returned via Quandl(), the data returned from quantmod() needs some manipulation to be ready for use. As we asked for four different prices, quantmod() returned a structure with four sets of data nested within. Execute the command head(stock_data) in the Console to see the four structures’ names. We will load three libraries - dplyr(Wickham, François, et al. 2020), broom(Robinson and Hayes 2020), and tidyr(Wickham and Henry 2020) - that will allow us to extract and manipulate the data into a form usable for analysis. We first pull the Tripwire data from the stock_data structure via the tidy() command, then convert the data from a ‘long’ format to a ‘wide’ format via the spread() command. library(broom) # tidy function library(tidyr) # spread function TRIP_tidy &lt;- tidy(TRIP) # tidy from broom package converts list to &quot;long&quot; data frame for DJI TRIP_wide &lt;- spread(TRIP_tidy, series, value) # spread from tidyr; converts long to wide using series values for new columns; that is, turn series row values to new columns We want to explore not the absolute price of Tripwire stock on a given day but the difference between its high and low price. We will need a new variable we will call Trip.Diff (to stay consistent with the naming convention of quantmod which does not following convention - dot notation is generally discouraged in R) and add this new variable to the data set via the mutate() function provided through the dplyr package. library(dplyr) # for mutate function (may already be loaded, but loading twice will not create an error) # The change in price during the day may be of interest TRIP_wide &lt;- TRIP_wide %&gt;% mutate(TRIP.Diff = TRIP.High - TRIP.Low) The first few rows of the Tripwire data are displayed in Table 2.2 as rendered through the package knitr(Xie 2020b). Table 2.2: First Few Rows of Financial Data Set index TRIP.Adjusted TRIP.Close TRIP.High TRIP.Low TRIP.Open TRIP.Volume TRIP.Diff 2020-06-04 22.01 22.01 23.20 21.93 22.99 4320500 1.270001 2020-06-05 25.49 25.49 26.60 23.17 23.17 10782400 3.430000 2020-06-08 25.45 25.45 26.74 24.97 26.56 4345000 1.770001 2020-06-09 23.86 23.86 24.75 23.65 24.68 3202200 1.100000 2020-06-10 22.22 22.22 23.83 22.03 23.66 3574000 1.799999 2020-06-11 19.90 19.90 21.15 19.85 20.73 5811000 1.300000 ##Social Media Data 2.2.3 Twitter Twitter requires registration in the form of an ‘application’ to use its API. Tweets that are public and can be searched via an API by keyword or account. Twitter does impose restrictions on the API use, e.g., limiting the number of calls per minute and the number of tweets returned. Twitter uses OAuth 1.0 tokens for authorization. You will need a consumer key, a consumer secret, an access token, and an access secret (basically, you need five character strings). Set up instructions can be found at https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html. We will use the command search_tweets() via the rtweet package to retrieve Twitter data by keywords. If you select a very broad term for search (e.g., “election”) you might only retrieve data from the last few hours. If you select a more obscure term (e.g., “kakorrhaphiophobia”) you still can only retrieve Tweets up to seven days old when using the free Twitter account. We first set parameter values for the call to search_tweets(), using the same technique for loading the token values, setting the search string to Caribbean and / or vacation, and limit the number of returned tweets to 1,000. The data returned is accessible through the vacation_tweets variable. library(httpuv) # for browser based twitter authentication library(rtweet) twitter_token &lt;- create_token( app = Sys.getenv(&quot;TWITTER_app&quot;), consumer_key = Sys.getenv(&quot;TWITTER_consumer_key&quot;), consumer_secret = Sys.getenv(&quot;TWITTER_consumer_secret&quot;), access_token = Sys.getenv(&quot;TWITTER_access_token&quot;), access_secret = Sys.getenv(&quot;TWITTER_access_secret&quot;)) tweet_search_string &lt;- &#39;Caribbean vacation &#39; # space for AND; this string also for hashtag terms and can include logic such as min_retweets:50 to return tweets that were retweeted a least 50 times Kmax_number_of_tweets = 1000 # Up to 18,000 vacation_tweets &lt;- search_tweets( q = tweet_search_string, n = Kmax_number_of_tweets, # Up to 18000 every 15 minutes include_rts = FALSE, # Retweets = tweet generated by &quot;retweet&quot; (recycle arrows), not quotes entering &quot;RT&quot; into text of one&#39;s tweets lang = &quot;en&quot;, # language: BCP 47 language identifier geocode = lookup_coords(&quot;usa&quot;), # for geo enabled tweets, most tweets do not include a geo code # since = from, # until = Sys.Date(), retryonratelimit = FALSE) # If ask for &gt; 18k if TRUE, will wait and resend request when eligible for next batch The Twitter API returns dozens of fields as shown below. While we will concentrate only on the actual tweet text, other fields that might be of interest is the number of retweets or the number of account followers. names(vacation_tweets) ## [1] &quot;user_id&quot; &quot;status_id&quot; ## [3] &quot;created_at&quot; &quot;screen_name&quot; ## [5] &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; ## [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; ## [11] &quot;is_quote&quot; &quot;is_retweet&quot; ## [13] &quot;favorite_count&quot; &quot;retweet_count&quot; ## [15] &quot;quote_count&quot; &quot;reply_count&quot; ## [17] &quot;hashtags&quot; &quot;symbols&quot; ## [19] &quot;urls_url&quot; &quot;urls_t.co&quot; ## [21] &quot;urls_expanded_url&quot; &quot;media_url&quot; ## [23] &quot;media_t.co&quot; &quot;media_expanded_url&quot; ## [25] &quot;media_type&quot; &quot;ext_media_url&quot; ## [27] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; ## [29] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; ## [31] &quot;mentions_screen_name&quot; &quot;lang&quot; ## [33] &quot;quoted_status_id&quot; &quot;quoted_text&quot; ## [35] &quot;quoted_created_at&quot; &quot;quoted_source&quot; ## [37] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; ## [39] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; ## [41] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; ## [43] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; ## [45] &quot;quoted_location&quot; &quot;quoted_description&quot; ## [47] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; ## [49] &quot;retweet_text&quot; &quot;retweet_created_at&quot; ## [51] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; ## [53] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; ## [55] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; ## [57] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; ## [59] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; ## [61] &quot;retweet_description&quot; &quot;retweet_verified&quot; ## [63] &quot;place_url&quot; &quot;place_name&quot; ## [65] &quot;place_full_name&quot; &quot;place_type&quot; ## [67] &quot;country&quot; &quot;country_code&quot; ## [69] &quot;geo_coords&quot; &quot;coords_coords&quot; ## [71] &quot;bbox_coords&quot; &quot;status_url&quot; ## [73] &quot;name&quot; &quot;location&quot; ## [75] &quot;description&quot; &quot;url&quot; ## [77] &quot;protected&quot; &quot;followers_count&quot; ## [79] &quot;friends_count&quot; &quot;listed_count&quot; ## [81] &quot;statuses_count&quot; &quot;favourites_count&quot; ## [83] &quot;account_created_at&quot; &quot;verified&quot; ## [85] &quot;profile_url&quot; &quot;profile_expanded_url&quot; ## [87] &quot;account_lang&quot; &quot;profile_banner_url&quot; ## [89] &quot;profile_background_url&quot; &quot;profile_image_url&quot; For convenience, we will pull the fields created_at, text, favorite_count, and retweet_count into another data set - vacation_tweets_df. #vacation_tweets_df$europe &lt;- grepl(&quot; Europe &quot;, rt$text, ignore.case = TRUE) # flag to indicate text include term &#39;Europe&#39; vacation_tweets_df &lt;- tibble(created_at = vacation_tweets$created_at, text = vacation_tweets$text, favorite_count = vacation_tweets$favorite_count, retweet_count = vacation_tweets$retweet_count) # tibble keeps the text from defaulting to factor Twitter also allows searches by account via the command get_timeline(). In the code below (which is not executred), tweets sent by the news service Business Wire would be captured and added to the vacation_tweets_df. NewsFromBW_tweets &lt;- get_timeline(&quot;NewsFromBW&quot;, n = Kmax_number_of_tweets) NewsFromBW_tweets_df &lt;- data.frame(created_at = NewsFromBW_tweets$created_at, text = NewsFromBW_tweets$text, favorite_count = NewsFromBW_tweets$favorite_count, retweet_count = NewsFromBW_tweets$retweet_count) vacation_tweets_df$source &lt;- &quot;General&quot; NewsFromBW_tweets_df$source &lt;- &quot;NewsFromBW&quot; vacation_tweets_df &lt;- rbind(NewsFromBW_tweets_df, vacation_tweets_df) vacation_tweets_df$text &lt;- gsub(&quot; Caribbean&quot;, &quot;&lt;span class=&#39;searchterms&#39;&gt; contract&lt;/span&gt;&quot;, vacation_tweets_df$text) The first few rows of the Twitter data are displayed below. Caution: The Twitter text may include characters R will interpret as escape characters which may adversely affect the bookdown generation. ## # A tibble: 6 x 4 ## created_at text favorite_count retweet_count ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2020-06-19 12:37:52 #LoveFL &amp;amp; ready to #Crui~ 0 0 ## 2 2020-06-19 11:56:32 @drcraigwax @HealthIs1 @RedC~ 0 0 ## 3 2020-06-19 11:01:25 Good Morning My Fellow Beach~ 0 0 ## 4 2020-06-14 11:01:09 Good Morning My Fellow Beach~ 0 0 ## 5 2020-06-17 11:00:33 Good Morning My Fellow Beach~ 1 0 ## 6 2020-06-15 11:01:27 Good Morning My Fellow Beach~ 0 0 No values appearing in the ‘favorite_count’ and ‘retweet_count’ may indicate values of zero 2.2.4 Reddit Reddit does not require an API key. Using the function get_reddit() from the package RedditExtractoR, we will obtain posts from the rubReddit stock picks, searching by the terms gold and _a few other terms silver. library(RedditExtractoR) reddit_data = get_reddit(search_terms = &quot;gold silver&quot;, subreddit = &quot;stock_picks&quot;, sort_by = &quot;new&quot;) # returns text as chr, not factor ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% The first few rows of the Reddit results are displayed below. ## [1] &quot;UBS Wealth Management Determines Ideal Selling Point for Gold:\\n\\nhttp://gold-prediction.com/2016/03/13/gold-news-ubs-wealth-management-determines-ideal-selling-point-for-gold/&quot; ## [2] &quot;Good to see this sub headed in a better direction. Best of luck!&quot; ## [3] &quot;Thanks! Hope to see you in the comments in the future!&quot; ## [4] &quot;Good luck!\\n\\n\\nPick of the month was good fun, I miss it.&quot; ## [5] &quot;Thank you! I&#39;ll restart the stock pick contest at the end of next month with some rules, stat-tracking, and fun flairs as prizes!&quot; ## [6] &quot;Maybe a bot for a monthly reminder message too. I always seem to remember to make my pick on the 2nd or 3rd. &quot; 2.3 Economic Data 2.3.1 Unemployment Insurance While an BEA package exists for pulling data, I have had some issues using it (it has not been updated in a couple of years; meanwhile, the public sites offering data undergo frequent updates). So instead we will use a more generic solution, a RESTful pull via the httr package (supporting cURL-like access to sites). We will ‘ask’ that the data is returned in a JSON (vs XML), a format that will need to be changed for use in R. Accordingly, we will use the package jsonlite to convert the data into a wide format called a data frame or tibble. The documentation for the packages can be found at: https://cran.r-project.org/web/packages/httr/ https://cran.r-project.org/web/packages/jsonlite The BEA data is available free of charge, but to registered users. Thus you will need to provide the unique key assigned to you by BEA as described at https://apps.bea.gov/API/signup/index.cfm. BEA methodologies can be found at https://www.bea.gov/resources/methodologies/nipa-handbook. A description of the data available can be found at https://apps.bea.gov/api/_pdf/bea_web_service_api_user_guide.pdf starting at page 16. The data set categories can be found at https://www.bea.gov/open-data). In the example provided below, we will obtain data from the NIPA database and store it in the variable called BEA_response. We check the return code hoping for a 200, which indicates the technical transaction was successful, but that does not necessarily mean data was returned. library(httr) # Normmally I would load all packages in a single chunk at the begiining of the notebook # Retrieve my BEA API Key stored in .Renviron project file BEA_API_Key &lt;- Sys.getenv(&quot;BEA_KEY&quot;) # Set parameter values for call BEA_Data_Set_Name &lt;- &quot;NIPA&quot; # also &quot;NIUnderlingDetail&quot;, &quot;FixedAssets&quot;, &quot;MNE&quot;, &quot;GDPbyIndustry&quot;, &quot;ITA&quot; , &quot;IIP&quot;, &quot;InputOutput&quot; , &quot;UnderlyingGDPbyIndustry&quot; BEA_Table_Name &lt;- &quot;T20600&quot; # Table T20600, Line 21 is Unemployment Insurance BEA_Year &lt;- &quot;2020&quot; BEA_Frequency = &quot;M&quot; # A = Annual, Q = Quarterly, M = Monthly - can use combination BEA_Results_Type &lt;- &quot;JSON&quot; # BEA_Data_Set_Name &lt;- &quot;NIUnderlyingDetail&quot; # BEA_Table_Name &lt;- &quot;U70205S&quot; # Line 12 is Truck Sales BEA_string &lt;- paste0(&quot;https://apps.bea.gov/api/data/?&amp;UserID=&quot;, BEA_API_Key, &quot;&amp;method=GetData&quot;, &quot;&amp;DataSetName=&quot;, BEA_Data_Set_Name, &quot;&amp;TableName=&quot;, BEA_Table_Name, &quot;&amp;Frequency=&quot;, BEA_Frequency, &quot;&amp;Year=&quot;, BEA_Year, &quot;&amp;ResultFormat=&quot;, BEA_Results_Type) # The parameters TableName, Frequency, and Year are required for the NIPA data sets. Other BEA data sets may require different parameters BEA_response &lt;- GET(BEA_string) # GET passes the string we created to the BEA web site. THe data set results returned are placed into the BEA_response variable http_status(BEA_response) # We Want code 200, which indicates successfull technical call, but not that data was returned ## $category ## [1] &quot;Success&quot; ## ## $reason ## [1] &quot;OK&quot; ## ## $message ## [1] &quot;Success: (200) OK&quot; Now we reformat the JSON data to a form usable by most R packages. library(jsonlite) BEA_response_content &lt;- content(BEA_response, &quot;text&quot;) # the content command &#39;is&#39;pulls&#39; the data from results BEA_response_struct &lt;- fromJSON(BEA_response_content) # Convert the JSON into a table-like format # THe BEA_response_struct variable is an elaborate structure that includes data and metadata (use the command str(BEA_response_struct) to see the structure). We just want to data. So we reference the data into a new variable. BEA_tibble &lt;- tibble(BEA_response_struct$BEAAPI$Results$Data) # Capture only the part of the table we want Table T20600 from the NIPA database includes a number of variables. We will pull the unemployment insurance (designated byLineNumber = 21) into a new variable called BEA_tibble_unemployment_ins. BEA_tibble_unemployment_ins &lt;- filter(BEA_tibble, LineNumber == 21) # Unemployment Insurance, filter is a dplyr function The first few rows of the BEA results are displayed in Table 2.3. Table 2.3: First Few Rows of Economic Data Set TableName SeriesCode LineNumber LineDescription TimePeriod METRIC_NAME CL_UNIT UNIT_MULT DataValue NoteRef T20600 W825RC 21 Unemployment insurance 2020M01 Current Dollars Level 6 26,478 NA T20600 W825RC 21 Unemployment insurance 2020M02 Current Dollars Level 6 26,158 NA T20600 W825RC 21 Unemployment insurance 2020M03 Current Dollars Level 6 69,631 NA T20600 W825RC 21 Unemployment insurance 2020M04 Current Dollars Level 6 430,107 NA References "],
["txt.html", "3 Text Analysis 3.1 Primary Functions", " 3 Text Analysis rmd version of file In this chapter we will perform simple text analysis - count and sentiment by word. Detailed guidance for text mining is provided at Text Mining with R(Xie 2020a). We first need to tokenize the text (i.e., break the text apart into smaller units for analysis). We will tokenize by word. Instead of tokenizing by word, you can tokenize by n-gram, e.g., pairs, triplets, etc. of words via vacation_tweets_tt2 &lt;- vacation_tweets_df %&gt;% unnest_tokens(bigram, text, token = \"ngrams\", n = 2). We will use the command unnest_tokens() from the tidytext(Robinson and Silge 2020) package to do our work. 3.1 Primary Functions unnest_tokens() via the tidytext package: Tokenize social media text anti_join() via the dplyr package: Remove ‘common’ words from text inner_join() via the dplyr package: Assign sentiment scores via lexicon available via the textdata package library(tidytext) # Create tokens (in our case, each word becomes a distinct value) vacation_tweets_tt &lt;- vacation_tweets_df %&gt;% unnest_tokens(word, text) # How do words with special characters like hashtags tokenized? vacation_reddit_tt &lt;- reddit_data[1:2,] %&gt;% unnest_tokens(word, comment) Stop words are “contextless” words that provide more clutter than information and are often removed from the text before analysis. stop_words include: a about actually and so forth. A data set of common stop words is included in the tidytext package. Below we use the dplyr command anti_join() that removes all word tokens appearing in the stop_words list. vacation_tweets_tt &lt;- vacation_tweets_tt %&gt;% anti_join(stop_words, by = c(&quot;word&quot; = &quot;word&quot;)) # anti_join is a dplyr function vacation_reddit_tt &lt;- vacation_tweets_tt %&gt;% anti_join(stop_words, by = c(&quot;word&quot; = &quot;word&quot;)) Technical Note: R is a functional(-like?) programming language (versus imperative programming, which is typically taught in school). A programming principle consistent with functional programming is immutability. Some would consider the code above poor practice as I am using the same variable on both sides of the equation versus creating a new variable, vacation_tweets_tt. Our analysis will be performed by date, that is, the field common to gold prices, stock prices, and social data is date. The date fields in the social data include time stamps - too granular of data for our purposes. We first convert the created_at from a date/timestamp to date field via the lubridate(Spinu, Grolemund, and Wickham 2020) package function as_date(). We then call the dplyr command group_by() to aggregate the rows by date, then count() the number of occurrences of each word by date. Note the %&gt;% - this is a piping syntax where output of the previous command becomes input for the next command. library(lubridate) # for the function as_date vacation_tweets_tt$created_at &lt;- as_date(vacation_tweets_tt$created_at) # By date, drop time stamp so we can generate a meaningful count by time tweet_word_count_by_date &lt;- vacation_tweets_tt %&gt;% group_by(created_at) %&gt;% count(word, sort = TRUE) ungroup(tweet_word_count_by_date) ## # A tibble: 1,243 x 3 ## created_at word n ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 2020-06-18 caribbean 25 ## 2 2020-06-18 https 23 ## 3 2020-06-18 t.co 23 ## 4 2020-06-18 vacation 21 ## 5 2020-06-11 caribbean 19 ## 6 2020-06-11 vacation 18 ## 7 2020-06-15 caribbean 18 ## 8 2020-06-15 vacation 15 ## 9 2020-06-11 https 14 ## 10 2020-06-11 t.co 14 ## # ... with 1,233 more rows vacation_reddit_tt$created_at &lt;- as_date(vacation_reddit_tt$created_at) reddit_word_count_by_date &lt;- vacation_reddit_tt %&gt;% group_by(created_at) %&gt;% count(word, sort = TRUE) ungroup(reddit_word_count_by_date) ## # A tibble: 1,243 x 3 ## created_at word n ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 2020-06-18 caribbean 25 ## 2 2020-06-18 https 23 ## 3 2020-06-18 t.co 23 ## 4 2020-06-18 vacation 21 ## 5 2020-06-11 caribbean 19 ## 6 2020-06-11 vacation 18 ## 7 2020-06-15 caribbean 18 ## 8 2020-06-15 vacation 15 ## 9 2020-06-11 https 14 ## 10 2020-06-11 t.co 14 ## # ... with 1,233 more rows Let’s combine the resulting Twitter and Reddit tokenized data. To keep track of the source, we will add a field called source with a value of Twitter or Reddit as appropriate (note that I use two different coding techniques to accomplish the same result). As the fields across the two sources are identical (because of the prior work narrowing the number of variables), we can simply append one data set to the other with a row bind command, rbind(). tweet_word_count_by_date &lt;- mutate(tweet_word_count_by_date, source = &quot;Twitter&quot;) # Adding field via mutate in dplyr reddit_word_count_by_date$source &lt;- &quot;Reddit&quot; # Adding field using base R social_data_count_by_date_word &lt;- rbind(tweet_word_count_by_date, reddit_word_count_by_date) Sentiment analysis, that is, are the social media positive or negative expressions, is popular text analysis approach. Those in the text mining domain use lexicons - lists of words with ‘assigned emotion’ including: scores, negative/positive ratings, intensity, and even subtlety (e.g., negative as fear vs. negative as anger). Lexicons available to use include bing, afinn, loughran, and nrc. We will use afinn from the textdata(Hvitfeldt 2020) package, which stores for each word in its data set a score from -5 (very negative) to 5 (very positive). To see the list, enter get_sentiments(\"afinn\") in the Console prompt. Our goal is to create one row for each date that reflects the opinions of the day based on the screened data. For each date, we will create an attribute for: number of words total sentiment count of negative words number of times the word ‘Paris’ was used We use an inner join to combine the two sets. Keep in mind that using an inner join means that any word in the social data data set unrecognized in the afinn data set will be dropped. You may want to keep these words - they may even be the most meaningful. For instance, we searched Twitter by the word Caribbean but this word in not in afinn. A simple way to accomplish this goal is to create your own afinn data set, e.g., afinn_custom &lt;- rbind(afinn, my_afinn) where my_afinn is a custom data set of words with your sentiment rankings of the words, as shown in the code chunk below. library(textdata) # Supports afinn, (-5 to 5), bing, loughran, and nrc lexicons my_afinn &lt;- tibble(word = c(&#39;Caribbean&#39;, &#39;vacation&#39;, &#39;beach&#39;), value = c(1, 3, 2)) afinn_custom &lt;- rbind(my_afinn, get_sentiments(&quot;afinn&quot;)) social_data_count_by_date_word &lt;- inner_join(social_data_count_by_date_word, afinn_custom) # add the afinn variable of value sentiment_data_by_date &lt;- social_data_count_by_date_word %&gt;% group_by(created_at) %&gt;% summarise(n = n(), # social media count total_sentiment = sum(value), # sum of sentiment negative_words = length(value[value &lt; 0]), # occurrences of negative words Paris_count = length(word[word == &#39;Paris&#39;]) # occurrences of the word &#39;Paris&#39; ) While Twitter only provides a week or so of data, you could collect and save the data over the course of the competition. You could save the data to your PC periodically via saveRDS(sentiment_data_by_date, paste0(\"sentiment_data_by_date_\", Sys.Date())). When you are ready to use the combined data set, open each RDS file via openRDS(&lt;name of RDF file&gt;), combine them back to the sentiment_data_by_date tibble via rbind(), then remove duplicates via the command sentiment_data_by_date &lt;- distinct(sentiment_data_by_date) (from the dplyr package). As you work through your text analysis, consider: Do you want to group by date? Or perhaps date and user? Do you want to analyze words or sentences? Do you need some additional preprocessing, e.g., remove certain words via custom_stopwords &lt;- tibble(word = c(\"retweet\", \"covid\", \"etc\")) References "],
["comb.html", "4 Combining Data 4.1 Primary Functions", " 4 Combining Data rmd version of file 4.1 Primary Functions inner_join() via the dplyr package: Combine data sets mutate() via the dplyr package: Create new columns of data select() via the dplyr package: Select subset of variables for analysis The following data sets are now available: London_Gold_AM: Daily price of gold sentiment_data_by_date: Daily Twitter and Reddit social data TRIP_wide: Daily Tripwire stock price from last two weeks BEA_tibble_unemployment_ins: First three months of 2020 unemployment insurance As we combined the Twitter and Reddit data sets in 3, we will want to combine all data sets into a single set; this is often the most convenient way to develop visualizations. We are analyzing by date, and each data set includes a date which, conveniently, is in the same format of YYYY-MM-DD. However, one theory might be that the social data sentiments precede changes to the gold prices. In other words, when we compare data, we want to look at Twitter as of Monday to examine the gold price on Tuesday. One convenient way to support this goal is simply to subtract one from the social media created_at date (a better way may be to add a new date field to all the data sets and adjust the values as wished). We also rename the variable storing the price of gold, USD (AM), as gold_morning_price. We use inner_join() again, first combining sentiment_data_by_date and London_Gold_AM into an all_data data set, then through another inner_join combining the all_data with the data set TRIP_wide. sentiment_data_by_date$created_at &lt;- sentiment_data_by_date$created_at -1 # Alter date to day before as we assume social media is a leading indicator London_Gold_AM &lt;- London_Gold_AM %&gt;% rename(gold_morning_price = &#39;USD (AM)&#39;) all_data &lt;- inner_join(sentiment_data_by_date, London_Gold_AM, by = c(&quot;created_at&quot; = &quot;Date&quot;)) # &#39;created_at&#39; will be the preserved key all_data &lt;- inner_join(all_data, TRIP_wide, by = c(&quot;created_at&quot; = &quot;index&quot;)) The unemployment insurance is reported on a monthly basis, so we do not have a row to row match by date. Let’s assume a three-month lag between the ‘macro’ condition of unemployment and the daily prices and sentiment. Thus, gold prices in May are associated with unemployment insurance in February. Thus, every price row by date in May will have the same BEA value from February. We need matching values across the data sets for successfully joining the data. To do so, we will ‘extract’ the month as an integer from the date fields. We will use the str_sub() command courtesy of the stringr(Wickham 2019b) package against the BEA data (and then add three to synch February with May). As we are using an inner_join(), any unmatched rows are lost. So, if we did not have any February dates in the BEA data set, we would have no data from the join. Keep that in mind as you chose your own lagging and leading relationships. library(stringr) library(lubridate) BEA_tibble_unemployment_ins &lt;- mutate(BEA_tibble_unemployment_ins, effect_month = as.integer(str_sub(TimePeriod, 6L, 7L)) + 3L) # TimePeriod is not a date variable, it is a string (char). str_sub lets us extract the month # as.integer turns the extracted month into a number. we then add three to the month to allow a match as a leading indicator all_data &lt;- mutate(all_data, effect_month = as.integer(month(created_at))) # Change month to integer to allow join all_data &lt;- inner_join(all_data, BEA_tibble_unemployment_ins, by = c(&quot;effect_month&quot; = &quot;effect_month&quot;)) # Adding the by is not necessary as the only common field between the two data sets is effect_date, but as data sets evolve, fields may be added that could inadvertently affect the join # Let&#39;s take a subset of only variables we plan to use final_data &lt;- select(all_data, created_at, n, total_sentiment, negative_words, Paris_count, gold_morning_price, TRIP.Diff, TRIP.Volume, DataValue) The final data set has one row for each word in each tweet or reddit post with the (repeated) associated gold prices and stock price changes for Tripwire as provided in Table 4.1. Table 4.1: Final Data Set created_at n total_sentiment negative_words Paris_count gold_morning_price TRIP.Diff TRIP.Volume DataValue 2020-06-10 20 26 4 0 1717.65 1.799999 3574000 69,631 2020-06-11 24 46 4 0 1731.90 1.300000 5811000 69,631 2020-06-12 18 40 2 0 1735.85 1.260000 3335100 69,631 2020-06-15 30 48 6 0 1710.40 1.470001 3235100 69,631 2020-06-16 18 20 6 0 1728.35 1.770000 4052900 69,631 2020-06-17 48 88 8 0 1717.30 1.609998 5809100 69,631 References "],
["viz.html", "5 Visualization 5.1 Primary Functions 5.2 Data Prep 5.3 Plots", " 5 Visualization rmd version of file 5.1 Primary Functions ggplot() via the ggplot2 package: Create plots While visualizations can be used for exploration and communication, for the competition, you will use plots to communicate, that is, share the relationships you have found in visual form. I did not find anything interesting between the price of gold, the daily spread of Tripwire prices, and the Twitter sentiment analysis. We will use the most popular visualization package in R, ggplot2(Wickham, Chang, et al. 2020), to create our graphs. Another popular R package used for visualizations is the lattice package. Base R also provides extensive plotting functions. ggplot uses themes as formatting templates (similar to a website’s use of css). The British Broadcasting Company provides a theme that renders a professional look to the graphs. The theme can be found in the package bbplot (Stylianou et al. 2020). 5.2 Data Prep library(ggplot2) library(bbplot) # devtools::install_github(&#39;bbc/bbplot&#39;) # Not all packages are available via CRAN; run once after installing devtools package Some visualizations are more readily generated from long formats rather than wide formats. In the chunk below, we use the dplyr select() function to pull the four values we want to use for visualizations into a new data set called final_data_long (still in a wide format). We want to plot values like gold prices and number of tweets together, but the scale of the two is dramatically different. To remedy this issue, we use the scale command on each value which ‘centers’ the data (makes the middle value zero and adjusts the other values accordingly) and the divides the values by the value’s standard deviation. The result are sets of values with similar values. Finally, we call the gather() function from tidyrr to transition from wide to long. final_data_long &lt;- final_data %&gt;% select(created_at, total_sentiment, TRIP.Diff, gold_morning_price) final_data_long$gold_morning_price &lt;- scale(final_data_long$gold_morning_price) final_data_long$total_sentiment &lt;- scale(final_data_long$total_sentiment) final_data_long$TRIP.Diff &lt;- scale(final_data_long$TRIP.Diff) final_data_long &lt;- final_data_long %&gt;% gather(source, value, total_sentiment, TRIP.Diff, gold_morning_price) The difference between the formats are best explained through displaying the data sets as wide in Table 5.1 and long in Table 5.2. Table 5.1: Wide Format created_at n total_sentiment negative_words Paris_count gold_morning_price TRIP.Diff TRIP.Volume DataValue 2020-06-10 20 26 4 0 1717.65 1.799999 3574000 69,631 2020-06-11 24 46 4 0 1731.90 1.300000 5811000 69,631 2020-06-12 18 40 2 0 1735.85 1.260000 3335100 69,631 2020-06-15 30 48 6 0 1710.40 1.470001 3235100 69,631 2020-06-16 18 20 6 0 1728.35 1.770000 4052900 69,631 2020-06-17 48 88 8 0 1717.30 1.609998 5809100 69,631 Table 5.2: Long Format created_at source value 2020-06-10 total_sentiment -0.5771964 2020-06-11 total_sentiment 0.2308785 2020-06-12 total_sentiment -0.0115439 2020-06-15 total_sentiment 0.3116860 2020-06-16 total_sentiment -0.8196188 2020-06-17 total_sentiment 1.9278359 5.3 Plots Note that I have turned warning off for these chunks. This option suppresses warnings from appearing on the rendered page / book. 5.3.1 Density The distribution of the values is usually insightful. You may discover that there is little variation (which usually makes the variable ‘boring’), that the distribution is normal, has extremes, or has multiple humps. Any of these results may help shape how you think about examining the relationships. ggplot(final_data_long) + geom_density(aes(x = value, group = source, fill = source), color = &#39;grey&#39;, alpha=0.4) + labs(title = &quot;Distribution of Values&quot;) + # subtitle = &quot;Sentiment Intensify by Tripwire Change Intensity&quot;) + bbc_style() + scale_fill_manual(values=c(&quot;gold&quot;, &quot;cyan1&quot;, &quot;coral1&quot;), labels=c(&quot;Gold&quot;, &quot;Total Sentiment&quot;, &quot;Tripwire Daily Change&quot;)) + theme(axis.text.y = element_blank()) + theme(axis.text.x = element_blank()) Figure 5.1: Distribution of Values 5.3.2 Line Next, let’s compare the changes over time using a line graph. Do we see values go up together, change in inverse, or seem to have no pattern at all? We use the wide data format, scale() within the code, and assign our own legend via the scale_colour_manual() command. ggplot(final_data, aes(x = created_at)) + geom_line(aes(y = scale(gold_morning_price), color=&quot;Gold&quot;), lty = 1, size = 2) + geom_line(aes(y = scale(total_sentiment), color=&quot;Sentiment&quot;), lty = 1, size = 1) + geom_line(aes(y = scale(TRIP.Diff), color=&quot;Tripwire&quot;), lty = 1, size = 1) + geom_hline(yintercept = 0, size = 1, color=&quot;#333333&quot;) + bbc_style() + # any changes to the bbc_style themes must be called after the bbc_style() call scale_colour_manual(values = c(Gold = &quot;gold&quot;, Sentiment = &quot;cyan1&quot;, Tripwire = &quot;coral1&quot;)) + theme(axis.text.y = element_blank()) + labs(title=&quot;Gold vs Drivers&quot;, subtitle = &quot;sentiment and Tripwire Daily Stock Price Differences&quot;) # You may receive warning messages indicating the Arial font is not available - that is OK Figure 5.2: Values over Time Note: The use of the legend above does not follow normal programming convention. We can use the long data and then use aes(color = TYPE) to autogenerate a legend.scale() is no longer helpful when used within ggplot2 as the full range of values across variables is in a single column, so scaling must be done in advance. Code using the long format to generate the same plot is provided (but not executed) below. ggplot(final_data_long, aes(x = created_at)) + geom_line(aes(y = value, color = source), lty = 1, size = 1) + geom_hline(yintercept = 0, size = 1, color=&quot;#333333&quot;) + bbc_style() + scale_color_manual(values=c(&quot;gold&quot;, &quot;cyan1&quot;, &quot;coral1&quot;), labels=c(&quot;Gold&quot;, &quot;Total Sentiment&quot;, &quot;Tripwire Daily Change&quot;)) + theme(axis.text.y = element_blank()) + labs(title=&quot;Gold vs Drivers&quot;, subtitle = &quot;sentiment and Tripwire Daily Stock Price Differences&quot;) 5.3.3 Bubbleplot A bubbleplot (a scatterplot with glyph sizes scaled according to a third variable) is another popular way to visually communicate relationships. In a scatterplot a variable is rendered in increasing order on the X axis and a second in increasing order on the Y axis. We again are looking for patterns - does a higher value of one variable We will use a combination of values for each bubbleplot axis. For the X axis, we will take the sentiment score and multiple it by the number of tweets for that date (as a measure of breadth of sentiment intensity). For the Y axis, we will take the difference between the high and low values on a particular date and multiple it by the trading volume, again as a measure of breadth of the market’s opinion of Tripwire. We will plot the price of gold for each date for the size of the glyph representing the X and Y coordinates. Finally, we impose a line to trace the change over the graph. Note that we do not use the date field. The first glyph may be from the last date in the data set - unlike the line graph, there is no longitudinal measures in the graph. We use the ggrepel(Slowikowski 2020) for the geom_text_repel() command which reduces text label overlap. library(ggrepel) # ggplot(data = final_data, aes(x = scale(total_sentiment*n), y = scale(TRIP.Diff*TRIP.Volume), size = gold_morning_price)) + geom_smooth(se = FALSE, color = &quot;black&quot;, linetype = 5, size = .5) + geom_point(color = &quot;black&quot;, fill = &quot;gold&quot;, shape = 21) + geom_hline(yintercept = 0, size = 1, color=&quot;#333333&quot;) + # geom_text(aes(label = paste0(&quot;$&quot;, round(gold_morning_price,0))), size = 6, nudge_x = 0.12, nudge_y = -0.07) + geom_text_repel(aes(label = paste0(&quot;$&quot;, round(gold_morning_price,0))), size = 6, nudge_x = 0.12, nudge_y = -0.07) + bbc_style() + theme(axis.title = element_text(size = 18)) + theme(axis.text.x = element_blank(), axis.text.y = element_blank()) + # override bbc_style axis title supression labs(x = &quot;Sentiment * Social Media Activity&quot;, y = &quot;Tripwire Daily Change * Volume&quot;) + # xlim(c(-1.5, 1.5)) + # ylim(c(-1.5, 1.5)) + guides(size = FALSE) + labs(title = &quot;Gold vs Drivers&quot;, subtitle = &quot;Sentiment Intensify by Tripwire Change Intensity&quot;) Figure 5.3: Bubbleplot of Values References "],
["publishing.html", "6 Publishing 6.1 Bibliography", " 6 Publishing rmd version of file The rmd files we have used for the project can be rendered as PDFs, Word documents, html files, and markdown (md) documents. Some rmd formatting syntax and options can be found at https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf. A tutorial for rmd can be found at https://rmarkdown.rstudio.com/lesson-1.html. The syntax is consistent with the Pandoc standards. The most important formatting decision will be whether code is executed and displayed. These choices can be controlled as: include = FALSE: blocks code and results from appearing but the code still executes results = HIDE: blocks code and results. echo = FALSE blocks the code from appearing but the code still executes (often used for visualizations). message = FALSE: blocks messages. warning = FALSE: blocks warnings. eval = FALSE: Code appears but is not executed. The **bookdown***[@R-bookdown] package should be installed (devtools::install_github(“rstudio/bookdown”)) to create the book version of the project (see https://bookdown.org/yihui/bookdown/ for details). You may also need install the package **TinyTeX** (which includes XeLaTeX), then run the commandtinytex::install_tinytex(). For further details, see https://yihui.name/tinytex/. Eachrmdfile contains one and only one chapter, and a chapter is defined by the first-level heading#`. Some issues to look for include: Social media includes escape codes Extraneous files in the project folder (especially _main). 6.1 Bibliography "],
["short.html", "7 Short Version 7.1 Prep 7.2 Step 1 - Microsoft Data 7.3 Step 2 - Twitter Data 7.4 Step 3 - Create Scores for the Twitter Data 7.5 Step 4 - Combine the Microsoft Stock Price Data and Twitter Data 7.6 Step 5 - Create a Line Graph", " 7 Short Version rmd version of file 7.1 Prep Load the packages listed below. Note that the bbplot package is not available via CRAN and must be downloaded via the install_github('bbc/bbplot'), available via the devtools package. library(quantmod) # for getSymbols() function library(broom) # for tidy() function library(tidyr) # for spread() function library(httpuv) # for browser based twitter authentication library(rtweet) # for search_tweets() function library(tidytext) # for unnest_tokens() function and stop_words data set library(dplyr) # for inner_join() and anti_join() functions library(lubridate) # for the as_date() function library(textdata) # for the afinn lexicon (sentiment scores for words) library(ggplot2) # for ggplot() function library(bbplot) # for plot formatting 7.2 Step 1 - Microsoft Data Let’s pull stock data on Microsoft for the last two weeks, i.e., from today (Sys.Date()) to fifteen days ago (Sys.Date() - 15). The Microsoft trading symbol is MSFT. getSymbols(Symbols = c(&quot;MSFT&quot;), src = &#39;yahoo&#39;, from = (Sys.Date() - 15), to = Sys.Date()) ## [1] &quot;MSFT&quot; We now have stock price data in a variable called MSFT. This variable is a data type called xts. We will change the data type to a type called tibble using the tidy() command from the broom package and the spread() command from the tidyr package. After executing the commands, we will have a table of data called MSFT_change with a change of the stock price for each date. MSFT_tidy &lt;- tidy(MSFT) MSFT_wide &lt;- spread(MSFT_tidy, series, value) MSFT_wide &lt;- mutate(MSFT_wide, change = MSFT.Close - MSFT.Open) MSFT_change &lt;- select(MSFT_wide, index, change) 7.3 Step 2 - Twitter Data We will get tweets referncing the words Microsoft and stock using the search_tweets() function from the rtweet package. Instructions to obtain access to Twitter data by creating a token can be found at https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html. After executing the commands, we will have a table of data called MSFT_tweets with the tweet text and the date/time that the tweet was originally created. twitter_token &lt;- create_token( app = Sys.getenv(&quot;TWITTER_app&quot;), consumer_key = Sys.getenv(&quot;TWITTER_consumer_key&quot;), consumer_secret = Sys.getenv(&quot;TWITTER_consumer_secret&quot;), access_token = Sys.getenv(&quot;TWITTER_access_token&quot;), access_secret = Sys.getenv(&quot;TWITTER_access_secret&quot;)) MSFT_tweets_all &lt;- search_tweets( q = &#39;Microsoft stock&#39;, # tweet must contain the words Microsoft and stock (but not necessarily used together) n = 15000, include_rts = FALSE, lang = &quot;en&quot;, retryonratelimit = FALSE) MSFT_tweets &lt;- select(MSFT_tweets_all, created_at, text) 7.4 Step 3 - Create Scores for the Twitter Data Let’s create a daily sentiment score for the tweets we pulled. The score will range from 5 (almost every reference is wonderful) to -5 (almost all the tweets are negative). We will use: unnest_tokens() function from the tidytext package to break each tweet into individual words anti_join from the dplyr package to remove the ‘boring’ words as_date() from the lubridate package to change the date format to allow us to combine the stock data and tweet data group_by() and count() functions from the dplyr package to take each sentiment score for each word and add them together for an overall score by date. After running the commands below, we will have a table of data called sentiment_data_by_date that contains for each date an overall sentiment score. MSFT_tweets_by_word &lt;- unnest_tokens(tbl = MSFT_tweets, output = word, input = text) # break tweets into words MSFT_tweets_by_word_no_stop_words &lt;- anti_join(x = MSFT_tweets_by_word, y = stop_words, by = c(&quot;word&quot; = &quot;word&quot;)) # remove common words MSFT_tweets_by_word_no_stop_words$created_at_date &lt;- as_date(x = MSFT_tweets_by_word_no_stop_words$created_at) # remove time stamp from date field social_data_count_by_date_word &lt;- inner_join(x = MSFT_tweets_by_word_no_stop_words, y = get_sentiments(&quot;afinn&quot;)) # add sentiment value for each word sentiment_data_by_date &lt;- group_by(social_data_count_by_date_word, created_at_date) %&gt;% summarise(total_sentiment = sum(value)) # total sentiment by date 7.5 Step 4 - Combine the Microsoft Stock Price Data and Twitter Data Now we combine the stock data set and Twitter data set in a data set called all_data. all_data &lt;- inner_join(x = sentiment_data_by_date, y = MSFT_change, by = c(&quot;created_at_date&quot; = &quot;index&quot;)) 7.6 Step 5 - Create a Line Graph Let’s compare the changes in stock price for a particular date to the sentiment expressed on Twitter for the same date. We will use the ggplot() function from the ggplot2 package. ggplot(all_data, aes(x = created_at_date)) + geom_line(aes(y = scale(change), color=&quot;Microsoft&quot;), lty = 1, size = 2) + geom_line(aes(y = scale(total_sentiment), color=&quot;Sentiment&quot;), lty = 1, size = 1) + geom_hline(yintercept = 0, size = 1, color=&quot;#333333&quot;) + bbc_style() + # any changes to the bbc_style themes must be called after the bbc_style() call scale_colour_manual(values = c(Microsoft = &quot;coral1&quot;, Sentiment = &quot;cyan1&quot;)) + theme(axis.text.y = element_blank()) + labs(title=&quot;Microsoft Stock Changes and Twitter Sentiment&quot;) Figure 7.1: Values over Time "],
["shortnm.html", "8 Short Version, No Social Media 8.1 Prep - Load the packages listed below. 8.2 Step 1 - Stock Data 8.3 Step 2 - Energy data 8.4 Step 3 - Combine the Stock Price Data and energy data 8.5 Step 4 - Create a plot", " 8 Short Version, No Social Media rmd version of file 8.1 Prep - Load the packages listed below. library(tibble) # for as_tibble() function library(quantmod) # for getSymbols() function library(Quandl) # for Quandl() function library(dplyr) # for mutate function library(ggplot2) # for ggplot() function In this abbreviated version of our economic analysis, we will analysis via plots the closing prices of ExxonMobil and Chevron to Crude Oil futures. We use eight functions in all: getSymbols() to obtain stock data as_tibble() to change the data type mutate() to add columns select() to obtain a subset of data rename() to rename columns Quandl() to obtain commodity prices rbind() to combine the data sets into one ggplot() to create a plot 8.2 Step 1 - Stock Data Let’s pull stock data on ExxonMobil and Chevron for the last month, i.e., from today (Sys.Date()) to thirty days ago (Sys.Date() - 30). The ExxonMobil trading symbol is XOM and the Chevron trading symbol is CVX. getSymbols(Symbols = c(&quot;XOM&quot;, &quot;CVX&quot;), src = &#39;yahoo&#39;, from = (Sys.Date() - 30), to = Sys.Date()) ## [1] &quot;XOM&quot; &quot;CVX&quot; We now have stock price data in a variable called XOM and CVX. This variable is a data type called xts. We will convert the data into a type called a tibble. As the date in the xts is a row name rather than included in the data, we will use the mutate() function to take the row names from the xts object and create a data column called date in our tibble. We will also use mutate() to add a column indicating the stock symbol, which will be useful when we later combine the data. We use the select() function to pull only the data columns we plan to use and the rename() to name the columns in a manner that will be consistent across our data sets. XOM_tibble &lt;- as_tibble(XOM) XOM_tibble &lt;- mutate(XOM_tibble, Date = index(XOM)) XOM_tibble &lt;- mutate(XOM_tibble, Name = &quot;ExxonMobil&quot;) XOM_tibble &lt;- select(XOM_tibble, Date, XOM.Close, Name) XOM_tibble &lt;- rename(XOM_tibble, Value = XOM.Close) CVX_tibble &lt;- as_tibble(CVX) CVX_tibble &lt;- mutate(CVX_tibble, Date = index(CVX)) CVX_tibble &lt;- mutate(CVX_tibble, Name = &quot;Chevron&quot;) CVX_tibble &lt;- select(CVX_tibble, Date, CVX.Close, Name) CVX_tibble &lt;- rename(CVX_tibble, Value = CVX.Close) 8.3 Step 2 - Energy data Create an account at Quandl to obtain an API key (basically, a long string of letters and numbers). We use the function Quandl() available after the Quandl package is loaded to obtain crude oil future prices from thirty days ago to the most recent date. Replace Sys.getenv(\"QUANDL_KEY\") with the API string provided by Quandl. So your code might look like api_key = \"XSyMWPWENLzhDZ-4Y8pj\". Make sure to use quotes around your key. Crude_oil &lt;- Quandl(code = &quot;CHRIS/CME_QM1&quot;, start_date = (Sys.Date() - 30), api_key = Sys.getenv(&quot;QUANDL_KEY&quot;)) Crude_oil &lt;- mutate(Crude_oil, Name = &quot;Oil&quot;) Crude_oil &lt;- select(Crude_oil, Date, Open, Name) Crude_oil &lt;- rename(Crude_oil, Value = Open) 8.4 Step 3 - Combine the Stock Price Data and energy data Now we combine the stock data set and commodity data set in a data set called all_data. As the three data sets have the same columns and column names (due to our previous work), we can use the rbind() function that in effect stacks the data. all_data &lt;- rbind(XOM_tibble, CVX_tibble, Crude_oil) 8.5 Step 4 - Create a plot Let’s compare the close of our stock prices on a particular date to the crude oil future openings. We will use the ggplot() function from the ggplot2 package. Note the last command is setting a theme. You can find additional themes at https://ggplot2.tidyverse.org/reference/ggtheme.html. After generating the plot, you should interpret the plot and explain why it is interesting or how it supports or contradicts your “theories”. ggplot(all_data, aes(x = Date)) + geom_line(aes(y = Value, color = Name), size = 2) + scale_x_date(date_breaks = &quot;1 week&quot;, date_labels = &quot;%B %d&quot;) + # Format x axis: Break each week, show full month and day scale_y_continuous(limits = c(0, max(all_data$Value))) + # Start the y axis at 0 instead of minimum value labs(title = &quot;Energy Stock Values and Crude Oil Futures&quot;, subtitle = paste0(&quot;from &quot;, format(Sys.Date() - 30, &quot;%B %d, %Y&quot;)), # dynamically include date in subtitle caption = &quot;Data from Yahoo and Quandl&quot;, color = &quot;Financial Instruments&quot;) + theme_classic() Figure 8.1: Values over Time "]
]
