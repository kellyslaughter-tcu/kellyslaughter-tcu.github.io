# Text Analysis {#txt}

<h5>[rmd version of file](03-text_analysis.Rmd)</h5> 

In this chapter we will perform simple text analysis - count and sentiment by word. Detailed guidance for text mining is provided at [Text Mining with R](https://www.tidytextmining.com/tidytext.html)[@xie2020]. We first need to tokenize the text (i.e., break the text apart into smaller units for analysis). We will tokenize by word. Instead of tokenizing by word, you can tokenize by n-gram, e.g., pairs, triplets, etc. of words via `vacation_tweets_tt2 <- vacation_tweets_df %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)`. We will use the command `unnest_tokens()` from the **tidytext**[@R-tidytext] package to do our work.

```{r Tokenize, message = FALSE, warning = FALSE}

library(tidytext)

# Create tokens (in our case, each word becomes a distinct value)
vacation_tweets_tt <- vacation_tweets_df %>% unnest_tokens(word, text) # How do words with special characters like hashtags tokenized?
vacation_reddit_tt <- reddit_data[1:2,] %>% unnest_tokens(word, comment) 

```

stop_words include:

* a
* about
* actually

and so forth. A data set of common stop words is included in the **tidytext** package. Below we use the **dplyr** command `anti_join()` that removes all word tokens appearing in the stop_words list.

```{r Remove-Stop-Words, message = FALSE, warning = FALSE}

vacation_tweets_tt <- vacation_tweets_tt %>% anti_join(stop_words, by = c("word" = "word")) # anti_join is a dplyr function
vacation_reddit_tt <- vacation_tweets_tt %>% anti_join(stop_words, by = c("word" = "word")) 

```

> Technical Note: R is a functional(-like?) programming language (versus imperative programming, which is typically taught in school). A programming principle consistent with functional programming is immutability. Some would consider the code above poor practice as I am using the same variable on both sides of the equation versus creating a new variable, `vacation_tweets_tt`. 

Our analysis will be performed by date, that is, the field common to gold prices, stock prices, and social data is date. The date fields in the social data include time stamps - too granular of data for our purposes. We first convert the `created_at` from a date/timestamp to date field via the **lubridate**[@R-lubridate] package function `as_date()`. We then call the **dplyr** command `group_by()` to aggregate the rows by date, then `count()` the number of occurrences of each word by date. 

Note the `%>%` - this is a piping syntax where output of the previous command becomes input for the next command.

```{r Text-Metrics-Twitter-Count-by-Date, message = FALSE, warning = FALSE}

library(lubridate) # for the function as_date

vacation_tweets_tt$created_at <- as_date(vacation_tweets_tt$created_at) # By date, drop time stamp so we can generate a meaningful count by time
tweet_word_count_by_date <- vacation_tweets_tt %>% group_by(created_at) %>% count(word, sort = TRUE) 
ungroup(tweet_word_count_by_date)


vacation_reddit_tt$created_at <- as_date(vacation_reddit_tt$created_at) 
reddit_word_count_by_date <- vacation_reddit_tt %>% group_by(created_at) %>% count(word, sort = TRUE) 
ungroup(reddit_word_count_by_date)


```

Let's combine the resulting Twitter and Reddit tokenized data. To keep track of the source, we will add a field called `source` with a value of _Twitter_ or _Reddit_ as appropriate (note that I use two different coding techniques to accomplish the same result). As the fields across the two sources are identical (because of the prior work narrowing the number of variables), we can simply append one data set to the other with a row bind command, `rbind()`.

```{r Combine-Twitter-and-Reddit, message = FALSE, warning = FALSE}

tweet_word_count_by_date <- mutate(tweet_word_count_by_date, source = "Twitter") # Adding field via mutate in dplyr
reddit_word_count_by_date$source <- "Reddit" # Adding field using base R

social_data_count_by_date_word <- rbind(tweet_word_count_by_date, reddit_word_count_by_date)

```

Sentiment analysis, that is, are the social media positive or negative expressions, is popular text analysis approach. Those in the text mining domain use lexicons - lists of words with 'assigned emotion' including: scores, negative/positive ratings, intensity, and even subtlety (e.g., negative as fear vs. negative as anger). Lexicons available to use include _bing_, _afinn_, _loughran_, and _nrc_. We will use _afinn_ from the **textdata**[@R-textdata] package, which stores for each word in its data set a score from -5 (very negative) to 5 (very positive). To see the list, enter `get_sentiments("afinn")` in the Console prompt.

Our goal is to create one row for each date that reflects the opinions of the day based on the screened data. For each date, we will create an attribute for: 

* number of words
* total sentiment
* count of negative words
* number of times the word 'Paris' was used

We use an inner join to combine the two sets. Keep in mind that using an inner join means that any word in the social data data set unrecognized in the _afinn_ data set will be dropped. You may want to keep these words - they may even be the most meaningful. For instance, we searched Twitter by the word _Caribbean_ but this word in not in _afinn_. A simple way to accomplish this goal is to create your own _afinn_ data set, e.g., `afinn_custom <- rbind(afinn, my_afinn)` where `my_afinn` is a custom data set of words with your sentiment rankings of the words, as shown in the code chunk below.

```{r Sentiment-Data, message = FALSE, warning = FALSE}

library(textdata) # Supports afinn, (-5 to 5), bing, loughran, and nrc lexicons

my_afinn <- tibble(word = c('Caribbean', 'vacation', 'beach'), value = c(1, 3, 2)) 
afinn_custom <- rbind(my_afinn, get_sentiments("afinn"))

social_data_count_by_date_word <- inner_join(social_data_count_by_date_word, afinn_custom) # add the afinn variable of value

sentiment_data_by_date <- social_data_count_by_date_word %>% 
                          group_by(created_at) %>% 
                          summarise(n = n(),                                    # social media count
                                    total_sentiment = sum(value),               # sum of sentiment
                                    negative_words = length(value[value < 0]),  # occurences of negative workds
                                    Paris_count = length(word[word == 'Paris']) # occurences of the word 'Paris'
                                    )

```

While Twitter only provides a week or so of data, you could collect and save the data over the course of the competition. You could save the data to your PC periodically via `saveRDS(sentiment_data_by_date, paste0("sentiment_data_by_date_", Sys.Date()))`. When you are ready to use the combined data set, open each RDS file via `openRDS(<name of RDF file>)`, combine them back to the `sentiment_data_by_date` tibble via `rbind()`, then remove duplicates via the command `sentiment_data_by_date <- distinct(sentiment_data_by_date)` (from the **dplyr** package).

As you work through your text analysis, consider:

* Do you want to group by date? Or perhaps date and user?
* Do you want to analyze words or sentences?
* Do you need some additional preprocessing, e.g., remove certain words via `custom_stopwords <- tibble(word = c("retweet", "covid", "etc"))`
